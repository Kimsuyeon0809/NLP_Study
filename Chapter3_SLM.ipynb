{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "p 121~138"
      ],
      "metadata": {
        "id": "ptFKl5KNqkVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter3. 언어 모델\n",
        "# - 통계 기반 전통적 언어 모델(Statistical Languagel Model, SLM)"
      ],
      "metadata": {
        "id": "AOZAJZ8PqWV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. 언어 모델이란?\n",
        "- 언어 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당하는 모델\n",
        "- 구분 : 통계를 이용한 방법 / 인공 신경망을 이용한 방법 ex) GPT, BERT\n",
        "- 단어 시퀀스에 확률을 할당 = 가장 자연스러운 시퀀스 찾아내는 모델 ex) 이전 단어 기반으로 다음 단어 예측\n",
        "- 언어 모델링 : 주어진 단어들로부터 아직 모르는 단어 예측 = 언어 모델이 이전 단어로 다음 단어 예측"
      ],
      "metadata": {
        "id": "-q4vyNSdqKeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 통계적 언어 모델 (Statistical Language Model, SLM)\n",
        "2-1) 조건부 확률\n",
        "- 각 단어는 문맥에 의해 이전 단어의 영향을 받음\n",
        "- 조건부 확률로 인한 문장의 확률 구할 수 있음\n",
        "\n",
        " <br/>\n",
        "\n",
        "2-2) 문장에 대한 확률\n",
        "- 문장의 확률 : 각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률의 곱\n",
        "\n",
        "ex) 𝑃 (An adorable little boy is spreading smiles) =\n",
        "𝑃 (An)×𝑃(adorable|An)×𝑃 (little|An adorable)×𝑃 (boy|An adorable little)×𝑃 (is|An adorable little boy)\n",
        "×𝑃 (spreading|An adorable little boy is) × 𝑃 (smiles|An adorable little boy is spreading)\n",
        "\n",
        " <br/>\n",
        "\n",
        "2-3) 카운트 기반의 접근\n",
        "- 이전 단어로부터 다음 단어에 대한 확률 구하기 -> 카운트 기반\n",
        "\n",
        "ex) An adorable little boy 가 나왔을 때, is 가 나올 확률인 𝑃 (is|An adorable little boy) 구하기\n",
        "\n",
        "𝑃(is|An adorable little boy) = count(An adorable little boy is) / count(An adorable little boy )\n",
        "\n",
        "  = An adorable little boy 뒤 is가 등장한 횟수 / An adorable little boy가 등장한 횟수\n",
        "\n",
        " <br/>\n",
        "\n",
        " 2-4) 카운트 기반 접근의 한계 - 희소 문제 (Sparsity Problem)\n",
        "- 언어 모델의 목표 : 많은 코퍼스를 훈련시켜 현실에서의 확률분포를 근사하는 것\n",
        "- 카운트 기반으로 접근하려면 코퍼스(훈련 데이터)의 양이 방대해야 함\n",
        "- 희소 문제 : 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제\n",
        "- 희소문제를 완화하기 위한 n-gram, 스무딩, 백오프 등의 일반화 기법이 존재하지만, 근본적 해결책이 되지 못해 통계적 언어 모델이 아닌 인공 신경망 언어 모델이 트렌드가 됨"
      ],
      "metadata": {
        "id": "Rlq9wvWayN56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. N-gram 언어 모델 (N‑gram Language Model)\n",
        "- 카운트 기반 통계적 접근을 사용하는 SLM의 일종이지만, 일부 단어만 고려하는 접근 방법 사용. 몇개의 단어를 고려하느냐가 N의 의미\n",
        " <br/>\n",
        "\n",
        " 3-1) 효과 : 카운트하지 못하는 경우의 감소\n",
        " - 문장이 길어질수록 코퍼스에 그 문장이 존재하지 않을 가능성이 높음(= 카운트가 0). 참고하는 단어를 줄이면 카운트 할 수 있을 가능성 증가\n",
        "\n",
        " ex) 𝑃 (is|An adorable little boy) ≈ 𝑃 (is|boy)\n",
        "\n",
        " <br/>\n",
        "\n",
        " 3-2) N-gram\n",
        " - 참고할 단어 수 정하기 위해 사용\n",
        " - n-gram : n개의 연속적인 단어 나열, n개의 단어 뭉치로 끊어서 하나의 토큰으로 간주\n",
        "\n",
        " ex) unigrams(= n이 1인 경우) : an, adorable, little, boy, is, spreading, smiles\n",
        "\n",
        " bigrams (= n이 2인 경우): an adorable, adorable little, little boy, boy is, is spreading, spreading smiles\n",
        "\n",
        " trigrams (= n이 3인 경우): an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles\n",
        "\n",
        " 4‑grams (= n이 4인 경우) : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles\n",
        " - n-gram으로 다음 단어를 예측할 때는, n-1개의 단어 고려해 예측\n",
        "\n",
        "  <br/>\n",
        "\n",
        "  3-3) N-gram 모델의 한계\n",
        "  - 앞의 몇 단어만 고려하니, 의도대로 문장을 끝맺음하지 못하는 경우 생김 -> 정확도 떨어짐\n",
        "  - 희소 문제 : 완화되긴 하지만, 여전히 희소 문제를 가짐\n",
        "  - n 선택의 trade-off 문제 : n이 큼 -> 모델 성능 증가 / 희소성 문제 증가, 모델 사이즈 커짐\n",
        "  n이 작음 -> 희소성 문제 완화 / 현실의 확률분포와 멀어짐 (성능 감소)\n",
        "  - trade-off 문제로, 정확도를 높이려면 n이 5이하일 것을 권장\n",
        "\n",
        "  <br/>\n",
        "\n",
        "  3-4) 적용 분야(Domain)에 맞는 코퍼스 수집\n",
        "  - 분야에 따라 특정 단어들의 확률 분포 다름 -> 언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스로 사용하면 성능 증가"
      ],
      "metadata": {
        "id": "GTJYVqnQS_W7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 한국어에서의 언어 모델\n",
        "4-1) 어순 중요 X\n",
        "\n",
        "= 다음에 어떤 단어든 등장할 수 있다 -> 모델이 다음 단어 예측하기 어려움\n",
        "\n",
        "  <br/>\n",
        "\n",
        "4-2) 교착어\n",
        "- 교착어 : 어근 + 접사로 단어의 기능이 결정되는 언어\n",
        "- 조사가 존재하기 때문에, 띄어쓰기 단위(어절)로 토큰화하면 단어의 수가 매우 많아짐 <br/> -> 토큰화를 통해 접사 / 조사를 분리하는 것이 중요\n",
        "\n",
        " <br/>\n",
        "\n",
        "4-3) 띄어쓰기가 잘 지켜지지 X\n",
        "- 한국어는 띄어쓰기가 없어도 의미가 전달됨, 띄어쓰기 규칙이 까다로움\n"
      ],
      "metadata": {
        "id": "mJBKkrwQX12I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 모델 평가 : 펄플렉서티 (Perplexity, PPL)\n",
        "- Perplexity : 헷갈리는 정도 -> 낮을수록 성능 좋은 모델\n",
        "- 문장의 길이로 정규화된 문장 확률의 역수\n",
        "- PPL은 분기계수 : 선택 가능한 경우의 수, 언어 모델이 특정 시점에서 평균 몇개의 선택지를 가지고 고민하는지를 의미\n",
        "\n",
        " ex) 모델의 PPL=10 -> 다음 단어 예측하는 모든 시점에서, 평균 10개의 단어를 가지고 고민한다는 의미 -> PPL 낮을수록 성능 좋음\n",
        "- PPL이 낮다 = 테스트 데이터 상에서 높은 정확도를 보인다 (사람이 느끼기에 좋은 모델이라는 것을 의미하지는 X)\n",
        "- 두 개 이상의 언어 모델을 비교할 때는, 도메인에 알맞는 동일한 데이터를 사용한 모델간의 비교여야 신뢰도 높음\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qqlkUaK9Y-ie"
      }
    }
  ]
}